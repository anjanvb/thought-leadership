## Bio
Anjanava Biswas is an adept technologist devoted to advancing the field of machine learning and artificial intelligence. With a strong knack for elucidating complex technical concepts, he has played a pivotal role in fostering a deeper understanding and appreciation for intelligent document processing through his various initiatives.

## [Technical Articles](#technical-articles)

1. [Integrate Amazon SageMaker Data Wrangler with MLOps workflows](https://aws.amazon.com/blogs/machine-learning/integrate-amazon-sagemaker-data-wrangler-with-mlops-workflows/)
   - This article discusses the integration of Amazon SageMaker Data Wrangler with MLOps workflows to automate the end-to-end ML lifecycle, highlighting the simplification of data preparation processes for data scientists and ML engineers&#8203;``oaicite:{"number":1,"metadata":{"type":"webpage","title":"Integrate Amazon SageMaker Data Wrangler with MLOps workflows | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/integrate-amazon-sagemaker-data-wrangler-with-mlops-workflows/","text":"As enterprises move from running ad hoc machine learning (ML) models to using AI/ML to transform their business at scale, the adoption of ML Operations (MLOps) becomes inevitable. As shown in the following figure, the ML lifecycle begins with framing a business problem as an ML use case followed by a series of phases, including data preparation, feature engineering, model building, deployment, continuous monitoring, and retraining. For many enterprises, a lot of these steps are still manual and loosely integrated with each other. Therefore, it’s important to automate the end-to-end ML lifecycle, which enables frequent experiments to drive better business outcomes. Data preparation is one of the crucial steps in this lifecycle, because the ML model’s accuracy depends on the quality of the training dataset.\n\n[Image: Machine learning lifecycle]\n\nData scientists and ML engineers spend somewhere between 70–80% of their time collecting, analyzing, cleaning, and transforming data required for model training. 【120†Amazon SageMaker Data Wrangler】 is a fully managed capability of 【121†Amazon SageMaker】 that makes it faster for data scientists and ML engineers to analyze and prepare data for their ML projects with little to no code. When it comes to operationalizing an end-to-end ML lifecycle, data preparation is almost always the first step in the process. Given that there are many ways to build an end-to-end ML pipeline, in this post we discuss how you can easily integrate Data Wrangler with some of the well-known workflow automation and orchestration technologies.","pub_date":null,"extra":{"cited_message_idx":29,"evidence_text":"source"}}}``&#8203;.

2. [Prevent Fake Account Sign-ups in Real Time with AI using Amazon Fraud Detector](https://aws.amazon.com/blogs/machine-learning/prevent-fake-account-sign-ups-in-real-time-with-ai-using-amazon-fraud-detector/)
   - The piece elaborates on utilizing Amazon Fraud Detector in real-time alongside Amazon Cognito custom authentication workflows to thwart fake account sign-ups, emphasizing early detection and prevention of fraudulent account creations as a robust defense against fraud and platform abuse&#8203;``oaicite:{"number":2,"metadata":{"type":"webpage","title":"Prevent fake account sign-ups in real time with AI using Amazon Fraud Detector | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/prevent-fake-account-sign-ups-in-real-time-with-ai-using-amazon-fraud-detector/","text":"Implementing an effective fraud prevention system is one of the top priorities for businesses that operate online web or mobile platforms. Businesses report millions of dollars of lost revenue each year due to fraud. Platform abuse and fraud prevention largely remain reactive, and is achieved by studying the profile behavior and transaction history of a user after they sign up. This approach is often manual, time-consuming, and expensive. Early detection and prevention of fraudulent account sign-ups on online platforms using artificial intelligence (AI) is an effective defense mechanism for combating fraud and abuse.\n\nThis post shows how you can use 【120†Amazon Fraud Detector】 in real time along with 【121†Amazon Cognito】 custom authentication workflows to prevent fake account sign-ups. Amazon Fraud Detector is a fully managed service that can identify potentially fraudulent online activities, such as creation of fake accounts or online payment fraud. Plus, you can use it without the need for any prior machine learning (ML) expertise. Unlike general-purpose ML packages, Amazon Fraud Detector is designed specifically to detect fraud.\n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile applications quickly and easily. It’s serverless, and can scale up to millions of users. I also discuss how you can use 【122†Amazon Pinpoint】 to track user sign-up flow events via user journeys and categorize users into segments. This is useful for user profiles and activity analysis in order to run effective marketing or promotional campaigns while maintaining a quality user experience.\n\n## Solution overview\n\nIn its general design, the solution uses an Amazon Fraud Detector supervised ML model along with a customized Amazon Cognito sign-up workflow to implement a real-time new user fraud prevention mechanism for online web and mobile applications. It also uses 【123†Amazon DynamoDB】 and 【124†AWS Lambda】 to customize the Amazon Cognito sign-up workflow. The following diagram illustrates the high-level architecture.","pub_date":null,"extra":{"cited_message_idx":35,"evidence_text":"source"}}}``&#8203;.

3. [Part 1: Intelligent Document Processing with AWS AI Services](https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/)
   - The article introduces IDP with AWS AI services for automating information extraction from various document types, aiming to replace manual, error-prone document processing with a faster, more accurate automated system, thus reducing overall costs and facilitating better business decisions&#8203;``oaicite:{"number":3,"metadata":{"type":"webpage","title":"Intelligent document processing with AWS AI services: Part 1 | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/","text":"Organizations across industries such as healthcare, finance and lending, legal, retail, and manufacturing often have to deal with a lot of documents in their day-to-day business processes. These documents contain critical information that are key to making decisions on time in order to maintain the highest levels of customer satisfaction, faster customer onboarding, and lower customer churn. In most cases, documents are processed manually to extract information and insights, which is time-consuming, error-prone, expensive, and difficult to scale. There is limited automation available today to process and extract information from these documents. Intelligent document processing (IDP) with AWS artificial intelligence (AI) services helps automate information extraction from documents of different types and formats, quickly and with high accuracy, without the need for machine learning (ML) skills. Faster information extraction with high accuracy helps in making quality business decisions on time, while reducing overall costs.\n\nAlthough the stages in an IDP workflow may vary and be influenced by use case and business requirements, the following figure shows the stages that are typically part of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP.\n\n[Image: Phases of intelligent document processing in AWS]\n\nIn this two-part series, we discuss how you can automate and intelligently process documents at scale using AWS AI services. In this post, we discuss the first three phases of the IDP workflow. In 【123†part 2】, we discuss the remaining workflow phases.","pub_date":null,"extra":{"cited_message_idx":41,"evidence_text":"source"}}}``&#8203;.

4. [Part 2: Intelligent Document Processing with AWS AI Services](https://aws.amazon.com/blogs/machine-learning/part-2-intelligent-document-processing-with-aws-ai-services/)
   - This article delves into the latter phases of the IDP workflow with AWS AI services, emphasizing transitioning from manual processing or legacy OCR solutions to an IDP pipeline for more accurate, faster, and cost-effective information extraction&#8203;``oaicite:{"number":4,"metadata":{"type":"webpage","title":"Intelligent document processing with AWS AI services: Part 2 | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/part-2-intelligent-document-processing-with-aws-ai-services/","text":"Amazon’s intelligent document processing (IDP) helps you speed up your business decision cycles and reduce costs. Across multiple industries, customers need to process millions of documents per year in the course of their business. For customers who process millions of documents, this is a critical aspect for the end-user experience and a top digital transformation priority. Because of the varied formats, most firms manually process documents such as W2s, claims, ID documents, invoices, and legal contracts, or use legacy OCR (optical character recognition) solutions that are time-consuming, error-prone, and costly. An IDP pipeline with AWS AI services empowers you to go beyond OCR with more accurate and versatile information extraction, process documents faster, save money, and shift resources to higher value tasks.\n\nIn this series, we give an overview of the IDP pipeline to reduce the amount of time and effort it takes to ingest a document and get the key information into downstream systems. The following figure shows the stages that are typically part of an IDP workflow.\n\n[Image: phases of intelligent document processing with AWS AI services.]\n\nIn this two-part series, we discuss how you can automate and intelligently process documents at scale using AWS AI services. In 【123†part 1】, we discussed the first three phases of the IDP workflow. In this post, we discuss the remaining workflow phases.","pub_date":null,"extra":{"cited_message_idx":47,"evidence_text":"source"}}}``&#8203;.

5. [Process Mortgage Documents with Intelligent Document Processing using Amazon Textract and Amazon Comprehend](https://aws.amazon.com/blogs/machine-learning/process-mortgage-documents-with-intelligent-document-processing-using-amazon-textract-and-amazon-comprehend/)
   - The article presents how IDP with AWS AI services automates and accelerates mortgage document processing, improving decision quality and reducing costs by efficiently handling the document-heavy workflow of the mortgage application process&#8203;``oaicite:{"number":5,"metadata":{"type":"webpage","title":"Process mortgage documents with intelligent document processing using Amazon Textract and Amazon Comprehend | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/process-mortgage-documents-with-intelligent-document-processing-using-amazon-textract-and-amazon-comprehend/","text":"Organizations in the lending and mortgage industry process thousands of documents on a daily basis. From a new mortgage application to mortgage refinance, these business processes involve hundreds of documents per application. There is limited automation available today to process and extract information from all the documents, especially due to varying formats and layouts. Due to high volume of applications, capturing strategic insights and getting key information from the contents is a time-consuming, highly manual, error prone and expensive process. Legacy optical character recognition (OCR) tools are cost-prohibitive, error-prone, involve a lot of configuring, and are difficult to scale. Intelligent document processing (IDP) with AWS artificial intelligence (AI) services helps automate and accelerate the mortgage application processing with goals of faster and quality decisions, while reducing overall costs.\n\nIn this post, we demonstrate how you can utilize machine learning (ML) capabilities with 【120†Amazon Textract】, and 【121†Amazon Comprehend】 to process documents in a new mortgage application, without the need for ML skills. We explore the various phases of IDP as shown in the following figure, and how they connect to the steps involved in a mortgage application process, such as application submission, underwriting, verification, and closing.","pub_date":null,"extra":{"cited_message_idx":53,"evidence_text":"source"}}}``&#8203;.

6. [Introducing One-Step Classification and Entity Recognition with Amazon Comprehend for Intelligent Document Processing](https://aws.amazon.com/blogs/machine-learning/introducing-one-step-classification-and-entity-recognition-with-amazon-comprehend-for-intelligent-document-processing/)
   - This article introduces a one-step document classification and real-time analysis feature in Amazon Comprehend for IDP, simplifying the direct analysis of semi-structured documents in native formats without the need for pre-processing to extract text&#8203;``oaicite:{"number":6,"metadata":{"type":"webpage","title":"Introducing one-step classification and entity recognition with Amazon Comprehend for intelligent document processing | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/introducing-one-step-classification-and-entity-recognition-with-amazon-comprehend-for-intelligent-document-processing/","text":"“Intelligent document processing (IDP) solutions extract data to support automation of high-volume, repetitive document processing tasks and for analysis and insight. IDP uses natural language technologies and computer vision to extract data from structured and unstructured content, especially from documents, to support automation and augmentation.” – Gartner\n\nThe goal of Amazon’s intelligent document processing (IDP) is to automate the processing of large amounts of documents using machine learning (ML) in order to increase productivity, reduce costs associated with human labor, and provide a seamless user experience. Customers spend a significant amount of time and effort identifying documents and extracting critical information from them for various use cases. Today, 【119†Amazon Comprehend】 supports classification for plain text documents, which requires you to preprocess documents in semi-structured formats (scanned, digital PDF or images such as PNG, JPG, TIFF) and then use the plain text output to run inference with your 【120†custom classification†docs.aws.amazon.com】 model. Similarly, for 【121†custom entity recognition†docs.aws.amazon.com】 in real time, preprocessing to extract text is required for semi-structured documents such as PDF and image files. This two-step process introduces complexities in document processing workflows.\n\nLast year, we 【122†announced support for native document formats】 with custom named entity recognition (NER) 【123†asynchronous jobs†docs.aws.amazon.com】. Today, we are excited to announce one-step document classification and real-time analysis for NER for semi-structured documents in native formats (PDF, TIFF, JPG, PNG) using Amazon Comprehend. Specifically, we are announcing the following capabilities:\n\n  * Support for documents in native formats for custom classification real-time analysis and asynchronous jobs\n  * Support for documents in native formats for custom entity recognition real-time analysis","pub_date":null,"extra":{"cited_message_idx":59,"evidence_text":"source"}}}``&#8203;.

7. [Introducing Self-Service Quota Management and Higher Default Service Quotas for Amazon Textract](https://aws.amazon.com/blogs/machine-learning/introducing-self-service-quota-management-and-higher-default-service-quotas-for-amazon-textract/)
   - The article announces self-service quota management for Amazon Textract via the AWS Service Quotas console, alongside higher default service quotas in select AWS Regions, facilitating better scaling of Amazon Textract usage and quicker processing of quota increase requests&#8203;``oaicite:{"number":7,"metadata":{"type":"webpage","title":"Introducing self-service quota management and higher default service quotas for Amazon Textract | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/introducing-self-service-quota-management-and-higher-default-service-quotas-for-amazon-textract/","text":"Today, we’re excited to announce self-service quota management support for 【119†Amazon Textract】 via the 【120†AWS Service Quotas†docs.aws.amazon.com】 console, and higher default service quotas in select AWS Regions.\n\nCustomers tell us they need quick turnaround times to process their requests for quota increases and visibility into their service quotas so they may continue to scale their Amazon Textract usage. With this launch, we’re improving Amazon Textract support for service quotas by enabling you to self-manage your service quotas via the Service Quotas console. In addition to viewing the default service quotas, you can now view your account’s applied custom quotas for a specific Region, view your historical utilization metrics per applied quota, set up alarms to notify when utilization approaches a threshold, and add tags to your quotas for easier organization. Additionally, we’re launching the 【121†Amazon Textract Service Quota Calculator†console.aws.amazon.com】, which will help you quickly estimate service quota requirements for your workload prior to submitting a quota increase request.","pub_date":null,"extra":{"cited_message_idx":65,"evidence_text":"source"}}}``&#8203;.

8. [Announcing Enhanced Table Extractions with Amazon Textract](https://aws.amazon.com/blogs/machine-learning/announcing-enhanced-table-extractions-with-amazon-textract/)
   - The article announces improvements in Amazon Textract's Tables feature for easier extraction of information from tabular structures in documents, auto-detecting titles, footers, section titles, and summary rows to simplify tabular data extraction&#8203;``oaicite:{"number":8,"metadata":{"type":"webpage","title":"Announcing enhanced table extractions with Amazon Textract | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/announcing-enhanced-table-extractions-with-amazon-textract/","text":"【119†Amazon Textract】 is a machine learning (ML) service that automatically extracts text, handwriting, and data from any document or image. Amazon Textract has a Tables feature within the 【120†AnalyzeDocument†docs.aws.amazon.com】 API that offers the ability to automatically extract tabular structures from any document. In this post, we discuss the improvements made to the 【121†Tables†docs.aws.amazon.com】 feature and how it makes it easier to extract information in tabular structures from a wide variety of documents.\n\nTabular structures in documents such as financial reports, paystubs, and certificate of analysis files are often formatted in a way that enables easy interpretation of information. They often also include information such as table title, table footer, section title, and summary rows within the tabular structure for better readability and organization. For a similar document prior to this enhancement, the Tables feature within `AnalyzeDocument` would have identified those elements as cells, and it didn’t extract titles and footers that are present outside the bounds of the table. In such cases, custom postprocessing logic to identify such information or extract it separately from the API’s JSON output was necessary. With this announcement of enhancements to the Table feature, the extraction of various aspects of tabular data becomes much simpler.\n\nIn April 2023, Amazon Textract introduced the ability to automatically detect titles, footers, section titles, and summary rows present in documents via the Tables feature. In this post, we discuss these enhancements and give examples to help you understand and use them in your document processing workflows. We walk through how to use these improvements through code examples to use the API and process the response with the 【122†Amazon Textract Textractor library†aws-samples.github.io】.","pub_date":null,"extra":{"cited_message_idx":71,"evidence_text":"source"}}}``&#8203;.

9. [Introducing Amazon Textract Bulk Document Uploader for Enhanced Evaluation and Analysis](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-textract-bulk-document-uploader-for-enhanced-evaluation-and-analysis/)
   - The article introduces the Bulk Document Uploader feature in Amazon Textract, allowing for effortless processing and evaluation of up to 150 documents at once without coding, helping users quickly gauge Amazon Textract's performance on their documents&#8203;``oaicite:{"number":9,"metadata":{"type":"webpage","title":"Introducing Amazon Textract Bulk Document Uploader for enhanced evaluation and analysis | AWS Machine Learning Blog","url":"https://aws.amazon.com/blogs/machine-learning/introducing-amazon-textract-bulk-document-uploader-for-enhanced-evaluation-and-analysis/","text":"【119†Amazon Textract】 is a machine learning (ML) service that automatically extracts text, handwriting, and data from any document or image. To make it simpler to evaluate the capabilities of Amazon Textract, we have launched a new Bulk Document Uploader feature on the Amazon Textract console that enables you to quickly process your own set of documents without writing any code.\n\nIn this post, we walk through when and how to use the Amazon Textract Bulk Document Uploader to evaluate how Amazon Textract performs on your documents.\n\n## Overview of solution\n\nThe Bulk Document Uploader should be used for quick evaluation of Amazon Textract for predetermined use cases. By uploading multiple documents simultaneously through an intuitive UI, you can easily gauge how well Amazon Textract performs on your documents.\n\nYou can upload and process up to 150 documents at once. Unlike the existing Amazon Textract console demos, which impose artificial limits on the number of documents, document size, and maximum allowed number of pages, the Bulk Document Uploader supports processing up to 150 documents per request and has the same document size and page limits as the Amazon Textract APIs. This makes it more efficient for you to evaluate a larger set of documents.\n\nThe Bulk Document Uploader outputs a standard Amazon Textract JSON response and CSV file. The results are provided in JSON format for easy programmatic analysis. Additionally, a human-readable CSV file with confidence scores is provided for simple comparison and evaluation of the extracted information.","pub_date":null,"extra":{"cited_message_idx":77,"evidence_text":"source"}}}``&#8203;.

10. [Deploying and Scaling Apache Solr on Kubernetes](https://aws.amazon.com/blogs/opensource/deploying-and-scaling-apache-solr-on-kubernetes/)
    - The article outlines deploying and scaling Apache Solr on Kubernetes using Amazon EKS for an enterprise-grade search platform, mitigating challenges associated with distributed deployment of Solr, making it highly available, scalable, and fault-tolerant with lower maintenance&#8203;``oaicite:{"number":10,"metadata":{"type":"webpage","title":"Deploying and scaling Apache Solr on Kubernetes | AWS Open Source Blog","url":"https://aws.amazon.com/blogs/opensource/deploying-and-scaling-apache-solr-on-kubernetes/","text":"【121†Apache Solr†solr.apache.org】 is an open source enterprise search platform built on 【122†Apache Lucene†lucene.apache.org】. Solr has been powering large-scale web and enterprise applications across industries such as retail, financial services, healthcare, and more. Its features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, and rich document handling.\n\nApache Solr’s distributed deployment comes with the challenges of setting up a highly available and scalable cluster. The operational overhead of maintaining a stable and elastic Solr cluster increases along with the volume of data and number of queries. Deploying Solr on Kubernetes helps remove some of these complexities so you can run a scalable, stable, and distributed Solr installation with low maintenance.\n\nIn this blog post, we explain how to deploy a highly available, scalable, and fault-tolerant enterprise-grade search platform with Apache Solr using 【123†Amazon Elastic Kubernetes Service (Amazon EKS)】. Amazon EKS is a managed service that can be used to run Kubernetes (K8s) on 【124†Amazon Web Services (AWS)】 without needing to install, operate, and maintain your own Kubernetes control plane or nodes. We also demonstrate how 【125†Prometheus†prometheus.io】 is used for monitoring, observability, alerting, and auto-scaling the deployment.","pub_date":null,"extra":{"cited_message_idx":83,"evidence_text":"source"}}}``&#8203;.

11. [Balancing Innovation with Safety & Privacy in the Era of Large Language Models (LLM)](https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a)
12. [NodeJS Runtime Environment with AWS Lambda Layers](https://medium.com/@anjanava.biswas/nodejs-runtime-environment-with-aws-lambda-layers-f3914613e20e)
13. [AWS Cognito User Pool Federation with OneLogin IDP](https://medium.com/@anjanava.biswas/aws-cognito-user-pool-federation-with-onelogin-idp-4b1962127b0b)
14. [Uploading Files to AWS S3 from React App using AWS Amplify](https://medium.com/@anjanava.biswas/uploading-files-to-aws-s3-from-react-app-using-aws-amplify-b286dbad2dd7)

## [Open Source Contributions](#open-source-contributions)

1. [AWS AI IDP Human-in-Loop](https://github.com/aws-samples/aws-ai-idp-human-in-loop)
2. [AWS AI PHI Deidentification](https://github.com/aws-samples/aws-ai-phi-deidentification)
3. [Amazon Textract IDP CDK Constructs](https://github.com/aws-samples/amazon-textract-idp-cdk-constructs)
4. [Langchain Experimental Comprehend Moderation](https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.comprehend_moderation)
5. [Amazon Fraud Detector End-to-End](https://github.com/aws-samples/amazon-fraud-detector-end-to-end)
6. [Amazon Fraud Detector with Cognito](https://github.com/aws-samples/amazon-fraud-detector-with-cognito)
7. [SM Data Wrangler MLOps Workflows](https://github.com/aws-samples/sm-data-wrangler-mlops-workflows)
8. [Amazon EKS Arch Apache Solr](https://github.com/aws-samples/amazon-eks-arch-apache-solr)
9. [LLM Safety Privacy](https://github.com/annjawn/llm-safety-privacy)

## Public Speaking

- AWS re:Invent 2022 Event Speaker - Intelligent Document Processing with AWS AI Services
- AWS Global Summit 2022 (San Francisco) Event Speaker - Intelligent Document Processing with AWS AI Services
- AWS AI/ML Solution Days (Palo Alto) Event Speaker - Enhancing Intelligent Document Processing with generative AI 
- AWS AI/ML Mega Modernization Week - Event Speaker - Prevent online fraud while ensuring a frictionless customer experience
- DataScience on AWS - Intelligent Document Processing with AWS AI Services and Generative AI
- Guest Speaker at “ML On AWS“ event hosted by Data Science Club Univ. of Texas Dallas Session Speaker for Machine Learning Lifecycle
